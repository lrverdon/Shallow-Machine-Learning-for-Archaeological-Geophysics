{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5d4c2e0-f758-4192-b280-8e5cdbb84548",
   "metadata": {},
   "source": [
    "# Please run the cells by pressing Shift + Enter\n",
    "First import the necesseary libraries and the napari viewer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "731d0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import vigra\n",
    "import napari\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import cv2 as cv\n",
    "import geopandas as gpd\n",
    "import pandas\n",
    "import os\n",
    "import json\n",
    "import h5py\n",
    "import tifffile\n",
    "import orthogonality_map\n",
    "import AnomalyEdge\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, jaccard_score, f1_score\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage import filters, morphology, transform as transf\n",
    "from scipy import ndimage as ndi, signal\n",
    "from skimage.segmentation import watershed\n",
    "from skimage.feature import peak_local_max\n",
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "from shapely.affinity import translate, scale, affine_transform\n",
    "from mlxtend.plotting import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec3559f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the GUI and start the napari viewer\n",
    "%matplotlib qt\n",
    "%gui qt\n",
    "viewer = napari.Viewer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94caf84",
   "metadata": {},
   "source": [
    "# 1. Import geophysical image \n",
    "Import the geophysical image to be analysed into the napari viewer using 'File -> Open File(s)'. If you would like to open a 3D dataset consisting of several horizontal slices, use the option: 'File -> Open Files as Stack'. This will allow you to select all horizontal slice images at once, and import the complete volume in napari. When importing the data volume in this way, an error message may appear when calculating the features in step 2.2. To avoid this, save the data volume as a 3D TIFF file by selecting 'File -> Save Selected Layer(s)', then remove the layer with the data volume from the layer list, and re-open the saved 3D TIFF file. 3D data can be visualised as horizontal slices, vertical slices (xz and yz plane) and as a volume; use the second and third button in the lower left corner of the viewer.<br> Run the code below to import the input data as a numpy array, extract the path of the input data and determine the number of dimensions for late use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84fe2afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = viewer.layers[0].data\n",
    "path = viewer.layers[0].source.path\n",
    "nrofdim = image.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a2d92b",
   "metadata": {},
   "source": [
    "# 2. Machine learning based pixel classification\n",
    "Pixel classification using the code below will usually be slower than in segmentation tools such as ilastik, LABKIT or Trainable Weka Segmentation. Usually, for 2D data the difference in speed is not very big. For 3D data, run times may be significantly longer and RAM usage may be significantly larger. If pixel segmentation has been performed by means of an external tool such as ilastik, please import the segmented image in step 3.1 below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc44403-b2cd-4005-bdf1-eb1597a51857",
   "metadata": {},
   "source": [
    "## 2.1. Load features\n",
    "If no pixel features have been calculated for your input data, please go to step 2.2.<br>\n",
    "If pixel features, feature names and scales for the input data have previously been saved, they can be loaded here. Please adjust the file name in the first line of each cell if necessary. The files should be in the same folder as the Jupyter Notebook, or their path should be specified in the code below. After loading the features, please go immediately to step 2.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68b8bbb1-04e7-4edf-853b-0c65eb5a19eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Features.pckl', 'rb')\n",
    "features = pickle.load(f)\n",
    "f.close()\n",
    "X = features.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24b5d715-0bad-45c4-becb-f2f452839c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Feature_names.pckl', 'rb')\n",
    "feature_names = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eff4e021-c4de-483f-9599-c891153280a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Feature_scales.pckl', 'rb')\n",
    "feature_scales = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2977a24-77b8-4eca-8e43-7f84a8e3a8cb",
   "metadata": {},
   "source": [
    "## 2.2. Select scales and features\n",
    "The feature scales below will be used for the pixel classification. These scales are defined by the standard deviation (sigma) of the kernel used for the gaussian smoothing which is carried out before the caluclation of the features. The scales (sigmas) below are appropriate for most geophysical datasets. You can add or remove scales by modifying the code line below. If you use a pretrained classifier, please use the same scales and features as were previously used for training the classifier.<br> If an error message appears when calculating the features saying that 'the kernel is longer than the line', this may indicate that one or more selected feature scales are too large for the data. This can happen for example in the case of 3D data, when the kernel (which is then also 3D) becomes too large in comparison with the number of horizontal slices. In that case, please remove the largest scale(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3298ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmas = [0.7, 1.0, 1.6, 3.5, 5.0, 10.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec4fc6d-869a-48c2-9db4-32779ee6e1af",
   "metadata": {},
   "source": [
    "The image features below will be used for the pixel classification. These are appropriate for most geophysical datasets. You can deselect features by deleting them or commenting them out by putting a '#' at the beginning of the line. Other features can be added and appended to the feature stack. Unless training and segmentation (section 2.6) are slow, it is recommended to use all image features below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "766dbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFloat = image.astype('float64')\n",
    "feature_stack = []\n",
    "feature_names = []\n",
    "feature_scales = []\n",
    "\n",
    "# Gaussian smoothing with sigma = 0.3\n",
    "gaussian_smoothing = vigra.filters.gaussianSmoothing(imageFloat, 0.3)\n",
    "feature_stack.append(gaussian_smoothing.ravel())\n",
    "feature_names.append('Gaussian smoothing')\n",
    "feature_scales.append(0.3)\n",
    "\n",
    "for sigma in sigmas:   \n",
    "    # Gaussian smoothing\n",
    "    gaussian_smoothing = vigra.filters.gaussianSmoothing(imageFloat, sigma)\n",
    "    feature_stack.append(gaussian_smoothing.ravel()) \n",
    "    feature_names.append('Gaussian smoothing')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    # Laplacian of Gaussian\n",
    "    laplacian_of_gaussian = vigra.filters.laplacianOfGaussian(imageFloat, sigma)\n",
    "    feature_stack.append(laplacian_of_gaussian.ravel()) \n",
    "    feature_names.append('Laplacian of Gaussian')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    # Gaussian Gradient Magnitude\n",
    "    gaussian_gradient_magnitude = vigra.filters.gaussianGradientMagnitude(imageFloat, sigma)\n",
    "    feature_stack.append(gaussian_gradient_magnitude.ravel()) \n",
    "    feature_names.append('Gaussian gradient magnitude')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    # Difference of Gaussians\n",
    "    difference_of_gaussians = vigra.filters.gaussianSmoothing(imageFloat, sigma) - vigra.filters.gaussianSmoothing(imageFloat, sigma * 0.66)\n",
    "    feature_stack.append(difference_of_gaussians.ravel())\n",
    "    feature_names.append('Difference of Gaussians')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    # Stucture tensor eigenvalues\n",
    "    structure_tensor_eigenvalues = vigra.filters.structureTensorEigenvalues(imageFloat, sigma, sigma * 0.5)[...,0]\n",
    "    feature_stack.append(structure_tensor_eigenvalues.ravel())\n",
    "    feature_names.append('Structure tensor eigenvalues')\n",
    "    feature_scales.append(sigma)\n",
    "\n",
    "    #Hessian of Gaussian eigenvalues\n",
    "    hessian_of_gaussian_eigenvalues = vigra.filters.hessianOfGaussianEigenvalues(imageFloat, sigma)[...,1]    \n",
    "    feature_stack.append(hessian_of_gaussian_eigenvalues.ravel())    \n",
    "    feature_names.append('Hessian of Gaussian eigenvalues')\n",
    "    feature_scales.append(sigma)\n",
    "features = np.asarray(feature_stack)\n",
    "X = features.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ad762c-f6d1-496b-8990-b3318f55a2c0",
   "metadata": {},
   "source": [
    "## 2.3. Save features\n",
    "If further classification tests will be carried out on the same input data in the future, it may be useful to store pixel features,  feature names and scales for later use. Please adjust the file names or path names in the first line of each cell if necessary (in the code below, files are saved in the same folder as the Jupyter Notebook). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bbbb3a5-99a0-4b4b-890c-73fbfec0aa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Features.pckl', 'wb')\n",
    "pickle.dump(features, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33f424b9-045b-4777-8e44-cc69968d46a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Feature_names.pckl', 'wb')\n",
    "pickle.dump(feature_names, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "60a53a41-44cd-456c-a373-db61e3b552d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Feature_scales.pckl', 'wb')\n",
    "pickle.dump(feature_scales, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785bb8f",
   "metadata": {},
   "source": [
    "## 2.4. View features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a4dd34-ec01-463a-9ca2-010b23b63c24",
   "metadata": {},
   "source": [
    "You can create an overview table of the features and scales by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2eecc517-1b49-46ef-9b48-c29b3f7cd378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Feature name  Scale\n",
      "0                Gaussian smoothing    0.3\n",
      "1                Gaussian smoothing    0.7\n",
      "2             Laplacian of Gaussian    0.7\n",
      "3       Gaussian gradient magnitude    0.7\n",
      "4           Difference of Gaussians    0.7\n",
      "5      Structure tensor eigenvalues    0.7\n",
      "6   Hessian of Gaussian eigenvalues    0.7\n",
      "7                Gaussian smoothing    1.0\n",
      "8             Laplacian of Gaussian    1.0\n",
      "9       Gaussian gradient magnitude    1.0\n",
      "10          Difference of Gaussians    1.0\n",
      "11     Structure tensor eigenvalues    1.0\n",
      "12  Hessian of Gaussian eigenvalues    1.0\n",
      "13               Gaussian smoothing    1.6\n",
      "14            Laplacian of Gaussian    1.6\n",
      "15      Gaussian gradient magnitude    1.6\n",
      "16          Difference of Gaussians    1.6\n",
      "17     Structure tensor eigenvalues    1.6\n",
      "18  Hessian of Gaussian eigenvalues    1.6\n",
      "19               Gaussian smoothing    3.5\n",
      "20            Laplacian of Gaussian    3.5\n",
      "21      Gaussian gradient magnitude    3.5\n",
      "22          Difference of Gaussians    3.5\n",
      "23     Structure tensor eigenvalues    3.5\n",
      "24  Hessian of Gaussian eigenvalues    3.5\n",
      "25               Gaussian smoothing    5.0\n",
      "26            Laplacian of Gaussian    5.0\n",
      "27      Gaussian gradient magnitude    5.0\n",
      "28          Difference of Gaussians    5.0\n",
      "29     Structure tensor eigenvalues    5.0\n",
      "30  Hessian of Gaussian eigenvalues    5.0\n"
     ]
    }
   ],
   "source": [
    "df = pandas.DataFrame(list(zip(feature_names, feature_scales)), columns = ['Feature name', 'Scale'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f5c38-a21a-4d33-8a3b-48a986da418c",
   "metadata": {},
   "source": [
    "You can view a feature by entering its number in the first line of the cell below. The number can be found by looking up the feature name and scale in the table above. If the data is 3D, please also enter the horizontal slice to be viewed. The image with the selected feature will open in a separate window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81824e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1b3019aec40>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_index = 20 # Select the feature to be viewed from the table above\n",
    "selectedSlice = 48 # For 3D data, select the slice to be viewed\n",
    "\n",
    "# Select feature\n",
    "selectedFeature = features[feature_index,:] \n",
    "if nrofdim == 2:\n",
    "    selectedFeatureReshaped = selectedFeature.reshape(image.shape)\n",
    "else:    \n",
    "    slice_size = np.prod(image.shape[1:3])\n",
    "    selectedFeatureSlice = selectedFeature[(selectedSlice * slice_size):((selectedSlice + 1) * slice_size)]\n",
    "    selectedFeatureReshaped = selectedFeatureSlice.reshape(image.shape[1:3])\n",
    "\n",
    "# Clip values to enhance visualisation\n",
    "mean_val = selectedFeatureReshaped.mean()\n",
    "std_val = selectedFeatureReshaped.std()\n",
    "clip_min, clip_max = mean_val - (3 * std_val), mean_val + (3 * std_val)\n",
    "selectedFeatureClip = np.clip(selectedFeatureReshaped, clip_min, clip_max)\n",
    "\n",
    "# Display feature\n",
    "plt.imshow(selectedFeatureClip,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac144c0-ee78-4c56-8f4e-095a1a9471cb",
   "metadata": {},
   "source": [
    "## 2.5. Load classifier\n",
    "If a classifier has been previously trained and saved, it can be loaded here. Please specify its name in the code below. It should be in the same folder as the Jupyter Notebook, or its path should be specified in the code below. After loading the classifier, please go to step 2.6.2 to apply it to the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27658fa-28c7-40f0-b55e-95fc55de3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Pixel_classifier.pckl', 'rb')\n",
    "classifier = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9847b-14b7-4aeb-86d7-fc818dfc9b1e",
   "metadata": {},
   "source": [
    "## 2.6. Iterative training and segmentation\n",
    "### 2.6.1. Training\n",
    "If no classifier has been loaded, please first run the code below if a file with labels (in H5 format) is available (for example if the data have been labeled in other software such as ilastik). Please adjust the file name in the first line if necessary. The file should be in the same folder as the Jupyter Notebook, or its path should be specified in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b8ab5a2-19b5-4b2e-a959-706b87cdcef6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'training_labels [1]' at 0x22006a6b2b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open H5 file with training labels and add it to the napari viewer\n",
    "filename = \"Labels.h5\"\n",
    "with h5py.File(filename, \"r\") as f:    \n",
    "    key = list(f.keys())[0]     \n",
    "    training_labels = f[key][...,0]\n",
    "viewer.add_labels(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097b49a4-6433-46b2-af9d-9849db533110",
   "metadata": {},
   "source": [
    "If no file with labels is available, please add an empty 'training_labels' layer to napari by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c105f34b-2b56-4ced-abcb-954b2f578249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'training_labels' at 0x220034c7250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels = np.zeros_like(image, dtype=np.uint8)\n",
    "viewer.add_labels(training_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7d8979",
   "metadata": {},
   "source": [
    "For the iterative training and classification, please follow the procedure below.\n",
    "1. Unless labels have been imported (see the first cell above), train the classifier by selecting the 'training_labels' layer in napari and adding labels to it, using the paint brush and other instruments (see the icons in the top left corner of the viewer under 'Layer controls'). This notebook requires two classes at the stage of pixel classification: background (= label 1) and archaeological structures (= label 2). Further classification occurs at the object level (see Section 3). Nevertheless, in theory it is possible to add more classes at this stage.  \n",
    "2. When the labels have been added, run the cells below to train the classifier and carry out the segmentation (step 2.6.2). After the segmentation has been completed, the result of the segmentation is automatically added as the 'segmentation' layer in napari. \n",
    "3. Additional labels can then be added in napari (in the 'training_labels' layer), and training and segmentation can be carried out again by running the steps below, in an iterative way, until the result of the segmentation is satisfactory.\n",
    "4. The labels can be saved by selecting the 'training_labels' layer and applying 'File -> Save Selected Layer(s)' in napari."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0ff981c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove 'segmentation' layer if it exists\n",
    "if 'segmentation' in viewer.layers:\n",
    "    viewer.layers.remove('segmentation')\n",
    "\n",
    "# Get and preprocess training labels\n",
    "training_labels = viewer.layers['training_labels'].data.ravel()\n",
    "mask = training_labels > 0\n",
    "Xsel, ysel = X[mask], training_labels[mask]\n",
    "\n",
    "# Train the classifier\n",
    "classifier = RandomForestClassifier(n_jobs=-1)\n",
    "classifier.fit(Xsel, ysel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de356b-34e4-4670-8847-9a15676caf6f",
   "metadata": {},
   "source": [
    "### 2.6.2. Segmentation\n",
    "Please run the code below to apply the trained classifier to the input data. The results of the segmentation are added as the 'segmentation' layer in napari. The segmentation is obtained by assigning to each pixel the class label with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af073562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'segmentation' at 0x1b37ca35fd0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = classifier.predict(X)\n",
    "segmentation = result.reshape(image.shape)\n",
    "viewer.add_labels(segmentation, opacity = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db3c38-f6eb-4135-966a-ed8f0b632ce4",
   "metadata": {},
   "source": [
    "## 2.7. Save classifier\n",
    "To save the classifier, please specify its name below. In the default code below, it is saved in the same folder as the Jupyter Notebook. Otherwise, its path should be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b641cb2b-69a9-4d02-8d5d-e3f543043016",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Pixel_classifier.pckl', 'wb')\n",
    "pickle.dump(classifier, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df342b",
   "metadata": {},
   "source": [
    "## 2.8. Apply hysteresis threshold and create objects\n",
    "### 2.8.1. Calculation of probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939857b1-35b8-4f49-9834-554b829cb3ae",
   "metadata": {},
   "source": [
    "In the segmentation in step 2.6.2, the class with the highest probability is assigned to each pixel. To perform a more refined segmentation, hysteresis thresholding can be applied to the probability map of a certain class (usually the 'archaeological structures' class). To do that, probabilities need to be calculated first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c03e7047",
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = classifier.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e5a85-04cb-44bb-b464-db3712b197b5",
   "metadata": {},
   "source": [
    "The code below adds the probability maps to the napari viewer (for the two classes 'background' and 'archaeological structures' as described in section 2.6.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f765672-a79d-485b-b29c-6f8a40d79fdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'Probability (class 2)' at 0x1b3742dde20>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability map for the first class ('background')\n",
    "probabilitymap = probabilities[:,0].reshape(image.shape)\n",
    "viewer.add_image(probabilitymap, opacity = 1.0, name = \"Probability (class 1)\")\n",
    "\n",
    "# Probability map for the second class ('archaeological structures')\n",
    "probabilitymap = probabilities[:,1].reshape(image.shape)\n",
    "viewer.add_image(probabilitymap, opacity = 1.0, name = \"Probability (class 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550d6b61-566e-438e-9666-cc12d2cf2aa5",
   "metadata": {},
   "source": [
    "### 2.8.2. Hysteresis thresholding\n",
    "For the application of hysteresis thresholding, there are two thresholds. Pixels that belong to the 'archaeological structures' class with a probability above the low threshold are only selected if they are connected to pixels above a higher, more stringent probability threshold (the high threshold). Both thresholds can be set in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2587f07a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'Segmentation after hysteresis thresholding (Class 2)' at 0x1b3018b3a60>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape the probability map so that it has the same dimensions as the geophysical image\n",
    "reshaped_probabilities = probabilities[:,1].reshape(image.shape)\n",
    "\n",
    "# Apply gaussian smoothing (the standard devitation of the gaussian kernel can be adjusted)\n",
    "smoothed_probabilities = vigra.filters.gaussianSmoothing(reshaped_probabilities, 0.5)\n",
    "\n",
    "# Hyseresis thresholding\n",
    "low = 0.5     # Enter the low threshold\n",
    "high = 0.6   # Enter the high threshold\n",
    "hyst = filters.apply_hysteresis_threshold(smoothed_probabilities, low, high)\n",
    "\n",
    "# Add the thresholded data to the napari viewer\n",
    "viewer.add_labels(hyst, name = \"Segmentation after hysteresis thresholding (Class 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06985bdf-8115-4f8d-a8eb-f52042db7d2c",
   "metadata": {},
   "source": [
    "### 2.8.3. Creation of individual objects\n",
    "Regions are labelled so that they can be used for subsequent object classification. Pixels receive the same label if they are neighbours in horizontal, vertical or diagonal direction. Connectivity can be changed to '1' if regions with the same label should only consist of pixels connected in horizontal or vertical (and not in diagonal) direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f79e9d68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'Objects (Class 2)' at 0x1b37fcb4400>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_hyst = label(hyst, connectivity = 2, background = 0)\n",
    "viewer.add_labels(labeled_hyst, name = \"Objects (Class 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad6f0b-4ea2-463d-8e21-c843dd7e0481",
   "metadata": {},
   "source": [
    "# 3. Object classification\n",
    "## 3.1. Import .h5 file with segmented objects\n",
    "If the pixel classification has been done in another software package (e.g. ilastik), please import the H5 file with the segmentation by running the code below. Please adjust the file name in the first line if necessary. The file should be in the same folder as the Jupyter Notebook, or its path should be specified in the code below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb6a4416-d21a-493e-a1ca-8395052f0931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'Segmentation' at 0x22006c3c790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open H5 file with labeled objects and add it to the napari viewer\n",
    "filename = \"Binary_image.h5\"\n",
    "with h5py.File(filename, \"r\") as f:    \n",
    "    key = list(f.keys())[0]     \n",
    "    hyst = f[key][...,0]         \n",
    "labeled_hyst = label(hyst)\n",
    "viewer.add_labels(labeled_hyst, name = \"Segmentation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f734523-2dde-42df-a06e-48838699a7ed",
   "metadata": {},
   "source": [
    "## 3.2. Watershed segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399fb4c4-5165-4a10-86b7-85e2da490f34",
   "metadata": {},
   "source": [
    "Often, the objects are too big for straightforward classification, because they encompass pixels belonging to more than one semantic class (for example parts of a wall and a floor). Watershed is applied to reduce the size of the regions and make them more semantically homogeneous. The size of the resulting regions can be controlled by the size of the footprint in the code below. If no watershed is to be used, please go immediately to the last cell of this section.<br>\n",
    "The code is based on: https://scikit-image.org/docs/stable/auto_examples/segmentation/plot_watershed.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c409bc6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'labeled_hyst_watershed' at 0x1b37fcbd310>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the labeled objects to 'int32' format \n",
    "labeled_hyst = labeled_hyst.astype(np.int32)\n",
    "\n",
    "# Compute the distance transform\n",
    "distance = ndi.distance_transform_edt(labeled_hyst.astype(np.int32))\n",
    "\n",
    "# Determine the footprint size (number of pixels or voxels)\n",
    "footprint_size = (30, 30) if nrofdim == 2 else (20, 20, 20)\n",
    "\n",
    "# Find local maxima\n",
    "coords = peak_local_max(distance, footprint=np.ones(footprint_size), labels=labeled_hyst)\n",
    "\n",
    "# Create a mask for the markers\n",
    "mask = np.zeros_like(distance, dtype=bool)\n",
    "mask[tuple(coords.T)] = True\n",
    "\n",
    "# Label the markers\n",
    "markers, _ = ndi.label(mask)\n",
    "\n",
    "# Apply watershed\n",
    "labeled_hyst_watershed = watershed(-distance, markers, mask=labeled_hyst)\n",
    "\n",
    "# Add the result to the viewer\n",
    "viewer.add_labels(labeled_hyst_watershed, opacity = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66937185-9d74-4424-b59b-508ae701bff6",
   "metadata": {},
   "source": [
    "If no watershed is used, please run the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8b0fa8c-b5a3-48ff-9c5f-10e9d1d66210",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_hyst_watershed = labeled_hyst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ef913b-7e6a-4bbd-ab3d-d98e0818b06a",
   "metadata": {},
   "source": [
    "## 3.3. Machine learning based object classification\n",
    "If only manual object classification will be carried out, please go directly to section 4.\n",
    "## 3.3.1. Selection of object features\n",
    "First run the code below to calculate properties of the objects created in section 2.8 or imported in section 3.1 (possibly after watershed segmentation; section 3.2) and create the object feature stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "053e8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = regionprops(labeled_hyst_watershed, intensity_image=image)\n",
    "object_feature_stack = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71065035-4eed-4dbc-b227-b9a39dfbea1e",
   "metadata": {},
   "source": [
    "### 3.3.1.1. Standard object features in scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf7da13-b871-4e37-a4b4-c5cba088347a",
   "metadata": {},
   "source": [
    "A large number of object features exist, see the list in https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops.<br> Not all features are supported for 3D data. If you are analysing 3D data, please run the code below for lists of supported and unsupported 3-D features (the code is based on: https://scikit-image.org/skimage-tutorials/lectures/three_dimensional_image_processing.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af6d73-f0a4-4452-9399-1305cc73588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "supported = [] \n",
    "unsupported = []\n",
    "\n",
    "for s in stats[0]:\n",
    "    try:\n",
    "        stats[0][s]\n",
    "        supported.append(s)\n",
    "    except NotImplementedError:\n",
    "        unsupported.append(s)\n",
    "\n",
    "print(\"Supported properties:\")\n",
    "print(\"  \" + \"\\n  \".join(supported))\n",
    "print()\n",
    "print(\"Unsupported properties:\")\n",
    "print(\"  \" + \"\\n  \".join(unsupported))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf72de8-2c3b-4933-afea-4bdd5d5ad690",
   "metadata": {},
   "source": [
    "A few combinations of object features are given below (for magnetometer and for GPR). Object features can be appended to the feature stack by adding a code line of the same form and including the name of the feature from the list created in the previous step or found on https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "994ba51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magnetometer data\n",
    "object_feature_stack.append(np.asarray([s.area for s in stats]))\n",
    "object_feature_stack.append(np.asarray([s.eccentricity for s in stats]))\n",
    "object_feature_stack.append(np.asarray([s.solidity for s in stats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e949cb20-4677-4e58-ac34-28a8b3c1d847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPR data\n",
    "#object_feature_stack.append(np.asarray([s.eccentricity for s in stats]))\n",
    "#object_feature_stack.append(np.asarray([s.orientation for s in stats]))\n",
    "object_feature_stack.append(np.asarray([s.solidity for s in stats]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaa46bee-d658-45bd-bbed-8af70c3ac940",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_feature_stack.append(np.asarray([s.area for s in stats]))\n",
    "object_feature_stack.append(np.asarray([s.centroid[0] for s in stats]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586647d3-8f35-49c9-bf43-de248b007acd",
   "metadata": {},
   "source": [
    "### 3.3.1.2. Combinations of standard object features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03963519-997b-4eba-8791-34bd362bfbc3",
   "metadata": {},
   "source": [
    "New object features can be created by combining standard features included in scikit-image. The examples below create new features from combinations of standard features, and append the newly created feature to the feature stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0471d0dd-1b17-42d2-a0ea-6b185c2e0355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code can be used to classify small dipoles in magnetometer data\n",
    "\n",
    "# Extract areas, intensity_min and intensity_max from stats\n",
    "areas = np.asarray([s.area for s in stats])\n",
    "intensity_min = np.asarray([s.intensity_min for s in stats])\n",
    "intensity_max = np.asarray([s.intensity_max for s in stats])\n",
    "\n",
    "# Calculate intensity difference and ratio\n",
    "ratio_intensity_size = (intensity_max - intensity_min) / areas\n",
    "\n",
    "# Append to object_feature_stack\n",
    "object_feature_stack.append(ratio_intensity_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20a72bbb-1d30-4627-a70b-b51fddf12ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a 'roundness' feature\n",
    "roundness = np.asarray([s.equivalent_diameter_area / s.axis_major_length for s in stats])\n",
    "object_feature_stack.append(roundness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee1593-eb49-431f-a8ac-bd3097c07285",
   "metadata": {},
   "source": [
    "### 3.3.1.3. New object features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ed47e-e9f6-436d-965e-4458e7e390be",
   "metadata": {},
   "source": [
    "It is also possible to create entirely new object features. The code below shows the example of two features than can be useful to classify objects belonging to linear structures having two dominant perpendicular orientations, for example walls. The following steps are required to create these object features.\n",
    "1. In the first code cell below, define the size of the rectangular structuring element (SE) which is representative for the linear structures, taking into account pixel size (e.g. if the distance between pixels is 0.05 m in x- and y-direction, a SE of 60 x 5 pixels equals 3 m x 0.25 m).\n",
    "2. Find the dominant orientation of the linear structures in the geophysical map. This orientation is found automatically by the algorithm using the Hough transform. For 3D volumes, please indicate in the first cell below which horizontal slice should be used for the calculation of the orientation (choose a slice where the structures are clear; the slice number can be seen in the bottom right corner of the napari viewer). For 2D images, indicate '0' as slice number.\n",
    "3. Calculate the orthogonality map, which shows to what extent each pixel belongs to a linear structure following the dominant orientation. The algorithm uses the rank-max opening (see the paper and supplementary data accompanying it for more details). Usually the 'rank' parameter in the code cell below can be left unchanged. The calculation is slow (it may take several hours for a 3D volume with many depth slices). The orthogonality map can be saved (and loaded again) for future use, by means of the second and third cells below. \n",
    "4. From the orthognality map, two object features ('mean orthogonality' and 'maximum orthogonality') are derived in the fourth code cell below, and appended to the feature stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0249ad6-81a9-431c-a0eb-3134070a21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of structuring element, the horizontal slice of the geophyiscal volume that should be used for deriving \n",
    "# the dominant orientation of the archaeological structures, and the rank to be used by the rank-max opening \n",
    "\n",
    "# Structuring element\n",
    "length = 60\n",
    "width = 5\n",
    "\n",
    "# Horizontal slice (this parameter is only used for 3D data)\n",
    "slicenr = 40 \n",
    "\n",
    "# Rank\n",
    "rank = 75 # mostly this can be left unchanged; if necessary, change this value to experiment with different ranks\n",
    "\n",
    "# Calculate orthogonality map\n",
    "orthogonalityMap = orthogonality_map.orthogonality_map(image, labeled_hyst_watershed, length, width, slicenr, rank, nrofdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dd781af-0329-493a-b7ed-ceee519cf203",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Orthogonality_map.pckl', 'wb')\n",
    "pickle.dump(orthogonalityMap, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef6e975-f87c-4db8-9d2a-98875dd38fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Orthogonality_map.pckl', 'rb')\n",
    "orthogonalityMap = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c31505a-133c-4bd4-81d7-f2fcb15d2b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'orthogonalityMap' at 0x1bd65c8e5e0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viewer.add_image(orthogonalityMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aee8be1-a992-421b-a086-25d26c9ba00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate properties of the ojects (the properties related to the intensity are based on the orthogonality map)\n",
    "stats_orthogonality = regionprops(labeled_hyst_watershed, intensity_image=orthogonalityMap)\n",
    "\n",
    "# Derive object features\n",
    "object_feature_stack.append(np.asarray([s.intensity_mean for s in stats_orthogonality]))  \n",
    "object_feature_stack.append(np.asarray([s.intensity_max for s in stats_orthogonality]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad8c34-a685-45b6-a067-ffd190da9cfe",
   "metadata": {},
   "source": [
    "### 3.3.1.4. Put the object feature stack in the right format so that it can be used for training the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5110712e",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_features = np.asarray(object_feature_stack)\n",
    "Xobj = object_features.T\n",
    "Xobj[Xobj == np.inf] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e31f9-fc20-40c8-b1e3-49ef10971bf5",
   "metadata": {},
   "source": [
    "## 3.3.2. Load classifier\n",
    "If an object classifier has been previously trained and saved, it can be loaded here. Please specify its name in the code below. It should be in the same folder as the Jupyter Notebook, or its path should be specified in the code below. After loading the classifier, please go immediately to step 3.3.3.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9a0489-4168-4fd3-baad-a19532e0bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Object_classifier.pckl', 'rb')\n",
    "classifierObj = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ddcbee",
   "metadata": {},
   "source": [
    "## 3.3.3. Interactive training and classification\n",
    "If no classifier has been loaded, please first run the code below to add an empty 'training_labels_object' layer to napari. Afterwards, if you would like to use raster labels for the classification, please go to step 3.3.3.1; if you would like to use vector shapes, please go to step 3.3.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d579b2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'training_labels_object' at 0x220071ecf70>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_labels_object = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "viewer.add_labels(training_labels_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad162e-2050-4fa8-a536-969c8cba002b",
   "metadata": {},
   "source": [
    "### 3.3.3.1. Creating training data by means of raster labels\n",
    "For the iterative training and classification, please carry out the procedure below.\n",
    "1. Train the classifier by selecting the 'training_labels_object' layer in napari and adding labels to it, and do so for as many classes as needed, using the paint brush and other instruments (see the icons in the top left corner of the viewer under 'Layer controls'). To label an object, it is sufficient that at least one pixel belonging to the object is labeled. If an object has received different labels, the label with the highest class number prevails.\n",
    "2. When the labels have been added, run the code below and then go to steps 3.3.3.3 and 3.3.3.4. \n",
    "3. After running the code in 3.3.3.3 and 3.3.3.4, additional labels can be added in napari (in the 'training_labels_object' layer), and classification can be carried out again by running the code line below and in steps 3.3.3.3 and 3.3.3.4, in an iterative way, until the result of the classification is satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9696194",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_object = viewer.layers['training_labels_object'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff2c27b-d348-43dd-a5f0-bffb9bc5ec37",
   "metadata": {},
   "source": [
    "### 3.3.3.2. Creating training data by means of vectors (polygons, lines etc.)\n",
    "Please first run the code below to add a 'shapes' layer to napari. Also indicate the class to which the objects covered by the shapes should belong (second line in the code below). All the objects should belong to the same class. If you are analysing 3D data, please also indicate the layer number in which you will label the objects (the layer number can be seen in the bottom right corner of the napari viewer when working with 3D data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "854b43e5-b9a2-49f4-acfd-e274b8be9476",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(name = 'shapes')\n",
    "classnumber = 1\n",
    "layernumber = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e60aed-db6a-428a-b7d4-d1f4c182faea",
   "metadata": {},
   "source": [
    "For the iterative training and classification using polygons, please carry out the procedure below.\n",
    "\n",
    "1. Select the 'shapes' layer in napari and add shapes to it, using the 'Add polygons' icon or other instruments (see the icons in the top left corner of the viewer, under 'Layer controls'). To label an object, it is sufficient that at least one pixel belonging to the object is covered by the shape.\n",
    "2. After having drawn the shapes, please run the code below, which converts the shapes to raster labels and adds them to the 'training_labels_object' layer.\n",
    "3. Additional shapes can be added in napari, using the same workflow: first create a new shapes layer and indicate the class to which the objects should be assigned (code cell above), then draw the shapes, and run the code below.\n",
    "4. Once all shapes have been added, please go to steps 3.3.3.3 and 3.3.3.4.\n",
    "5. After running the code in 3.3.3.3 and 3.3.3.4, additional vector labels can be added in napari, and classification can be carried out again by running the code below and in steps 3.3.3.3 and 3.3.3.4, in an iterative way, until the result of the classification is satisfactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5bb266e1-8054-4a63-b3b5-baf22c363046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shapes from napari viewer and then remove the layer in napari\n",
    "shapes = viewer.layers['shapes']\n",
    "viewer.layers.remove('shapes')\n",
    "\n",
    "# Convert shapes to labels\n",
    "labels = shapes.to_labels(image.shape) if nrofdim == 2 else np.zeros(image.shape, dtype=np.uint8)\n",
    "if nrofdim == 3:\n",
    "    labels[layernumber, :, :] = shapes.to_labels(image.shape[1:3])\n",
    "\n",
    "# Add the labels to the 'training_labels_object' layer\n",
    "training_labels_object[labels > 0] = classnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f4a13a-6daa-4cf8-bbf3-8557c89815d7",
   "metadata": {},
   "source": [
    "### 3.3.3.3. Training the classifier\n",
    "After the training data has been created in steps 3.3.3.1 and/or 3.3.3.2, please run the code below to train the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6837921c-5d60-46c7-9371-1a3d830b955c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>RandomForestClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.ensemble.RandomForestClassifier.html\">?<span>Documentation for RandomForestClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove existing 'class_image' layer in the napari viewer\n",
    "if 'class_image' in viewer.layers: \n",
    "    viewer.layers.remove('class_image')\n",
    "yobj = training_labels_object.ravel()\n",
    "\n",
    "# Function to calculate the median of the non-zero elements in the objects. The non-zero elements correspond to the pixels \n",
    "# where the user has drawn a label. When different labels have been drawn over an object, the median of the labelled\n",
    "# pixels defines the label of the object.\n",
    "def medianOfNonZeros(regionmask, training_labels_object):\n",
    "    training_labels_object_mask = training_labels_object[regionmask]\n",
    "    non_zero_elements = training_labels_object_mask[training_labels_object_mask != 0]\n",
    "    if non_zero_elements.size == 0:\n",
    "        return 0\n",
    "    elif (non_zero_elements.size) % 2 == 0:\n",
    "        return np.median(non_zero_elements[:-1])\n",
    "    else:\n",
    "        return np.median(non_zero_elements)\n",
    "\n",
    "# Calculate properties of the objects (including the median of their non-zero pixels)\n",
    "annotation_stats = regionprops(labeled_hyst_watershed, intensity_image=training_labels_object, extra_properties=(medianOfNonZeros,))\n",
    "annotated_class = np.asarray([s.medianOfNonZeros for s in annotation_stats])\n",
    "maskObj = annotated_class > 0\n",
    "\n",
    "# Train the random forest object classifier\n",
    "classifierObj = RandomForestClassifier(n_jobs=-1)\n",
    "classifierObj.fit(Xobj[maskObj], annotated_class[maskObj])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c061a7-895f-465a-9175-a8ab5a422045",
   "metadata": {},
   "source": [
    "### 3.3.3.4. Applying the classifier to the entire dataset\n",
    "After the classification has been completed, the result of the classification is automatically added as the 'class_image' layer in napari. The labels can be saved by selecting the 'training_labels_object' layer and applying 'File -> Save Selected Layer(s)' in napari.\n",
    "The classification can be saved by selecting the 'class_image' layer and applying 'File -> Save Selected Layer(s)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42174f16-0ba3-4810-8668-0b197411f7e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image' at 0x2a0a040e610>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the classifier to the complete dataset\n",
    "resultObj = classifierObj.predict(Xobj)\n",
    "\n",
    "# Create an empty classification image\n",
    "class_image = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "\n",
    "# Add the classified objects to the classification image\n",
    "for k, result in enumerate(resultObj):\n",
    "    if result > 0:\n",
    "        if nrofdim == 2:\n",
    "            class_image[labeled_hyst_watershed == k + 1] = result\n",
    "        else:\n",
    "            startz, starty, startx, stopz, stopy, stopx = annotation_stats[k].bbox\n",
    "            portion = (slice(startz, stopz), slice(starty, stopy), slice(startx, stopx))\n",
    "            class_image_portion = class_image[portion]\n",
    "            labeled_portion = labeled_hyst_watershed[portion]\n",
    "            class_image_portion[labeled_portion == k + 1] = result\n",
    "            class_image[portion] = class_image_portion\n",
    "\n",
    "# Add classification image to the napari viewer\n",
    "viewer.add_labels(class_image, opacity=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509359e8-4034-4d7e-9e03-7e6f4ec624bb",
   "metadata": {},
   "source": [
    "## 3.3.4. Save classifier\n",
    "To save the classifier, please specify its name below. In the default code below, it is saved in the same folder as the Jupyter Notebook. Otherwise, its path should be specified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b6b65ce2-7a73-4d3d-87a6-65a47dfc2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Object_classifier.pckl', 'wb')\n",
    "pickle.dump(classifierObj, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40f563e",
   "metadata": {},
   "source": [
    "# 4. Manual classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0455602-786f-4125-b6f3-9a6911e9821c",
   "metadata": {},
   "source": [
    "## 4.1. Manual assignment of all objects to one class\n",
    "If the vast majority of the objects belong to one class, it may be more straightforward to assign them manually to this class than to perform machine learning based object classification (section 3.3). In that case, run the code below and first adjust the class number in the first line. The objects not belonging to this class can then be assigned to their correct class in steps 4.2, 4.3 and 4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a55f418f-2a6f-44df-a7b4-1bcb628397fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image [1]' at 0x1910fea82b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classnumber = 1\n",
    "class_image = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "class_image[labeled_hyst_watershed > 0] = classnumber\n",
    "viewer.add_labels(class_image, opacity = 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d89e3-c3ec-4341-abba-2a8fe61079be",
   "metadata": {},
   "source": [
    "## 4.2. Manual object re-classification \n",
    "Objects wrongly classified by the classifier in section 3.3, or manually in section 4.1, can be manually assigned to a different class. Please first run the code below to add an empty 'manual_labels_object' labels layer to napari. Afterwards, if you would like to use raster labels for the classification, please go to step 4.2.1; if you would like to use vector shapes, please go to step 4.2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7074d200-7f2a-4b5b-a64e-5c70a723c230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'manual_labels_object' at 0x178147abd60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_labels_object = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "viewer.add_labels(manual_labels_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf24e717-0c9d-497f-9c4d-a9ac0ac2c364",
   "metadata": {},
   "source": [
    "### 4.2.1. By means of raster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a092c4a3-d4dc-467b-8aa9-dc129458ed25",
   "metadata": {},
   "source": [
    "For the manual object classification, please carry out the procedure below.\n",
    "1. Select the 'manual_labels_object' layer in napari and add labels to it, using the paint brush and other instruments (see the icons in the top left corner of the viewer under 'Layer controls'). To label an object, it is sufficient that at least one pixel belonging to the object is labeled. \n",
    "2. After having labelled the objects, please run the code line below, and go to step 4.2.3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e498851",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_labels_object = viewer.layers['manual_labels_object'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d476cb3a-eb50-4ab4-add5-9b11442cf12b",
   "metadata": {},
   "source": [
    "### 4.2.2. By means of vectors (polygons, lines etc.)\n",
    "Please first run the code below to add a 'shapes' layer to napari. Also indicate the class to which the objects covered by the shapes should belong (second line in the code below). All the objects should belong to the same class. When working with 3D data, please also indicate the horizontal slice number in which you label the objects (the slice number can be seen in the bottom right corner of the napari viewer when working with 3D data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aa779c7-a47a-481e-aa3a-5e7c5ff5995c",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(name = 'shapes')\n",
    "classnumber = 6\n",
    "slicenumber = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efff16-f898-4e82-9870-ebe317c3e8f7",
   "metadata": {},
   "source": [
    "For the manual object classification using vector labels, please carry out the procedure below. \n",
    "1. Select the 'shapes' layer in napari and add shapes to it, using the 'Add polygons' or other instruments (see the icons in the top left corner of the viewer, under 'Layer controls'). To label an object, it is sufficient that at least one pixel belonging to the object is covered by the shape.\n",
    "2. After having drawn the shapes, please run the code below, which converts the shapes to raster labels and adds them to the 'manual_labels_object' layer.\n",
    "3. Additional shapes can be added in napari, using the same workflow: first create a new 'shapes' layer and indicate the class to which the objects should be assigned (code cell above), then draw the shapes, and run the code below. \n",
    "4. Once all labels have been added, please go to step 4.2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1197df02-cd7b-4d23-b6d2-4d651913c000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shapes from napari viewer and then remove the layer in napari\n",
    "shapes = viewer.layers['shapes']\n",
    "viewer.layers.remove('shapes')\n",
    "\n",
    "# Convert shapes to labels\n",
    "labels = shapes.to_labels(image.shape) if nrofdim == 2 else np.zeros(image.shape, dtype=np.uint8)\n",
    "if nrofdim == 3:\n",
    "    labels[slicenumber, :, :] = shapes.to_labels(image.shape[1:3])\n",
    "\n",
    "# Add the labels to the 'manual_labels_object' layer\n",
    "manual_labels_object[labels > 0] = classnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb422f1-bac1-4181-9a84-0c6e783150b5",
   "metadata": {},
   "source": [
    "### 4.2.3. Applying the modifications made\n",
    "The modifications made in step 4.2.1 or 4.2.2 are applied to the 'class_image' layer by running the code below. If an object has received different labels, the median of the labelled pixel values determines the class number.\n",
    "The labels can be saved by selecting the 'manual_labels_object' layer and applying 'File -> Save Selected Layer(s)' in napari. The classification can be saved by selecting the 'class_image' layer and applying 'File -> Save Selected Layer(s)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "552ad622-278e-4cf6-b6d0-95ae573b39f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image' at 0x2a0a2d32fd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove existing 'class_image' layer in the napari viewer\n",
    "if 'class_image' in viewer.layers: \n",
    "    viewer.layers.remove('class_image') \n",
    "\n",
    "# Function to calculate the median of the non-zero elements in the objects. The non-zero elements correspond to the pixels \n",
    "# where the user has drawn a label. When different labels have been drawn over an object, the median of the labelled\n",
    "# pixels defines the label of the object.\n",
    "def medianOfNonZeros(regionmask, manual_labels_object):\n",
    "    manual_labels_object_mask = manual_labels_object[regionmask]\n",
    "    non_zero_elements = manual_labels_object_mask[manual_labels_object_mask != 0]\n",
    "    if non_zero_elements.size == 0:\n",
    "        return 0\n",
    "    elif (non_zero_elements.size) % 2 == 0:\n",
    "        return np.median(non_zero_elements[:-1])\n",
    "    else:\n",
    "        return np.median(non_zero_elements)\n",
    "\n",
    "# Calculate properties of the objects (including the median of their non-zero pixels)\n",
    "annotation_stats_man = regionprops(labeled_hyst_watershed, intensity_image=manual_labels_object, extra_properties=(medianOfNonZeros,))\n",
    "annotated_class_man = np.asarray([s.medianOfNonZeros for s in annotation_stats_man])\n",
    "\n",
    "# Add the manually re-classified objects to the classification image\n",
    "for k, s in enumerate(annotation_stats_man):\n",
    "    if annotated_class_man[k] > 0:\n",
    "        if nrofdim == 2:\n",
    "            starty, startx, stopy, stopx = s.bbox\n",
    "            portion = (slice(starty, stopy), slice(startx, stopx))\n",
    "        else:            \n",
    "            startz, starty, startx, stopz, stopy, stopx = s.bbox\n",
    "            portion = (slice(startz, stopz), slice(starty, stopy), slice(startx, stopx))\n",
    "        class_image_portion = class_image[portion]\n",
    "        labeled_portion = labeled_hyst_watershed[portion]\n",
    "        class_image_portion[labeled_portion == k + 1] = annotated_class_man[k]\n",
    "        class_image[portion] = class_image_portion\n",
    "\n",
    "# Add the classification image to the napari viewer\n",
    "viewer.add_labels(class_image, opacity=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edfdd6c-d826-4987-ae5c-b31d7456eb01",
   "metadata": {},
   "source": [
    "## 4.3. Pixel-based correction of existing objects (no modification of background)\n",
    "Besides the manual re-classification of entire objects (section 4.2), parts of objects can be assigned to a different class, by labelling pixels belonging to that object. Please first run the code below to add an empty 'manual_labels_pixel' layer to napari. Afterwards, if you would like to use raster labels for the classification, please go to step 4.3.1; if you would like to use vector shapes, please go to step 4.3.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea7a5ed1-cd09-4230-ae0e-97e69944c537",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'manual_labels_pixel' at 0x1787b66c880>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_labels_pixel = np.zeros(image.shape[:3], dtype=np.uint8)\n",
    "viewer.add_labels(manual_labels_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1048c1-9cb6-41dc-be89-b3057f857814",
   "metadata": {},
   "source": [
    "### 4.3.1. By means of raster labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9b5e66-1d09-43a8-b075-0ddd59d7157b",
   "metadata": {},
   "source": [
    "For the manual pixel-based correction, please carry out the procedure below.\n",
    "1. Add labels to the 'manual_labels_pixel' layer in napari, using the paint brush and other instruments. This can be done for all existing classes. Only the pixels that belong to an already existing object, will be re-classified. Where the label is painted outside an existing object (i.e. in the background), it remains background. \n",
    "2. When the labels have been added, please run the code below, and go to step 4.3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5612330d-4018-4e87-9a5e-a1de97ba8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_labels_pixel = viewer.layers['manual_labels_pixel'].data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792cee91-6f97-4946-bcd1-ec8965098d7b",
   "metadata": {},
   "source": [
    "In 3D data, the correction will only apply to the horizontal slice which was annotated. If you want to apply it to all slices, please run the code below. Indicate the slice which was annotated in the first line of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "522f7d22-5caa-46fc-ae3c-6f5ac88c06c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'manual_labels_pixel [1]' at 0x1dc1e4bf040>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slicenumber = 6\n",
    "manual_labels_pixel = np.tile(manual_labels_pixel[slicenumber, :, :], (image.shape[0], 1, 1))\n",
    "viewer.add_labels(manual_labels_pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df902af1-61b0-49f0-b198-9aedeac5f0ea",
   "metadata": {},
   "source": [
    "### 4.3.2. By means of vector polygons\n",
    "Please first run the code below to add a 'shapes' layer to napari. Also indicate the class to which the pixels covered by the shapes should belong (second line in the code below). All re-classified pixels should belong to the same class. When working with 3D data, please also indicate the slice number in which you label the objects (the slice number can be seen in the bottom right corner of the napari viewer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3ffb1c8f-8254-4a59-9683-7824f2e909a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(name = 'shapes')\n",
    "classnumber = 2\n",
    "slicenumber = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54231233-0990-415d-a9aa-b7910bd8a721",
   "metadata": {},
   "source": [
    "For the manual pixel-based classification using vector labels, please carry out the procedure below.\n",
    "\n",
    "1. Select the 'shapes' layer in napari and add shapes to it, using 'Add polygons' or other instruments (see the icons in the top left corner of the viewer, under 'Layer controls'). Only the pixels that belong to an already existing object, will be re-classified. Where the shape extends outside an existing object (i.e. in the background), the background remains unchanged.\n",
    "2. After having drawn the shapes, please run the code below, which converts the shapes to raster labels and adds them to the 'manual_labels_pixel' layer.\n",
    "3. For 3D data, the correction will only apply to the horizontal slice which was annotated if the variable 'applyToAllSlices' is set to 'N' (first line of the code below). If you want the correction to apply to all slices, please set this variable to 'Y'.\n",
    "4. Additional shapes can be added in napari, using the same workflow: first create a new 'shapes' layer and indicate the class to which the objects should be assigned (code cell above), then draw the shapes, and run the code below.\n",
    "5. Once all labels have been added, please go to step 4.3.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "32abdaf8-7934-4de1-8651-d62b8ff40d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "applyToAllSlices = 'Y' # For 3D data: apply changes to all slices ('Y') or only to the annotated slice ('N')\n",
    "\n",
    "# Get shapes from napari viewer and then remove the layer in napari\n",
    "shapes = viewer.layers['shapes']\n",
    "viewer.layers.remove('shapes')\n",
    "\n",
    "# Convert shapes to labels\n",
    "labels = shapes.to_labels(image.shape) if nrofdim == 2 else np.zeros(image.shape, dtype=np.uint8)\n",
    "if nrofdim == 3:\n",
    "    labels_slice = shapes.to_labels(image.shape[1:3])\n",
    "    if applyToAllSlices == 'N':\n",
    "        labels[slicenumber,:,:] = labels_slice\n",
    "    else:\n",
    "        labels = np.tile(labels_slice,[image.shape[0],1,1])\n",
    "\n",
    "# Add the labels to the 'manual_labels_pixel' layer\n",
    "manual_labels_pixel[labels > 0] = classnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7300bdd-e07e-453e-87cd-750c7a2e419f",
   "metadata": {},
   "source": [
    "### 4.3.3. Applying the modifications made\n",
    "The modifications made in step 4.3.1 or 4.3.2 are applied to the 'class_image' layer by running the code below. The labels can be saved by selecting the 'manual_labels_pixel' layer and applying 'File -> Save Selected Layer(s)' in napari. The classification can be saved by selecting the 'class_image' layer and applying 'File -> Save Selected Layer(s)'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20bd8e13-4ad5-4ef3-a909-873ce93b4da6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image' at 0x17851bb2250>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if 'class_image' in viewer.layers: \n",
    "    viewer.layers.remove('class_image')\n",
    "\n",
    "# Add the manually corrected pixels to the classification image\n",
    "class_image[manual_labels_pixel > 0] = manual_labels_pixel[manual_labels_pixel > 0]\n",
    "\n",
    "# Any pixels labeled outside existing objects are discarded\n",
    "class_image[labeled_hyst_watershed == 0] = 0\n",
    "\n",
    "# Add the classification image to the napari viewer\n",
    "viewer.add_labels(class_image, opacity = 0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a251e-f409-40fb-ae6d-001d8d7898c3",
   "metadata": {},
   "source": [
    "## 4.4. Pixel-based correction (including background), or creation of new objects\n",
    "Whereas in section 4.3 pixels can be re-classified only if they belong to an existing object, in this section any pixel can be re-classified (including the background) by labelling it. In this way also new objects can be created. \n",
    "1. If you would like to use raster labels for this correction, simply add labels to the 'class_image' layer, using the paint brush and other instruments. Where the label is painted outside existing objects (i.e. in the background class), these pixels are assigned to the new class. It may be useful to save the 'class_image' layer first, by selecting that layer and applying 'File -> Save Selected Layer(s)' in napari (because changes can only be undone manually with the eraser and paint brush).\n",
    "2. If you would like to use vector shapes for the correction, please first run the code below to add a 'shapes' layer to napari. Also indicate the class to which the pixels covered by the shapes should belong (second line in the code below). All re-classified pixels should belong to the same class. When working with 3D data, please also indicate the horziontal slice number in which you label the objects (the slice number can be seen in the bottom right corner of the napari viewer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b43bd02f-f269-4d5a-b7e8-7ee49eb9fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer.add_shapes(name = 'shapes')\n",
    "classnumber = 6\n",
    "slicenumber = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdc2ee1-e33d-4944-9aa8-d4714f46b600",
   "metadata": {},
   "source": [
    "For the manual pixel-based correction (including background) using vector labels, please carry out the procedure below.\n",
    "\n",
    "1. Select the 'shapes' layer in napari and add shapes to it, using the 'Add polygons' or other instruments (see the icons in the top left corner of the viewer, under 'Layer controls'). Where the shape is drawn outside existing objects (i.e. in the background class), these pixels are assigned to the new class. In this way, this tool can be used for the creation of new objects.\n",
    "2. After having drawn the shapes, please run the code below, which converts the shapes to raster labels and adds them to the 'class_image' layer.\n",
    "3. Additional shapes can be added in napari, using the same workflow: first create a new 'shapes' layer and indicate the class to which the objects should be assigned (code cell above), then draw the shapes, and run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d379286c-cd6c-45e0-b6c5-22a9a91345bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get shapes from napari viewer and then remove the layer in napari\n",
    "shapes = viewer.layers['shapes']\n",
    "viewer.layers.remove('shapes')\n",
    "\n",
    "# Convert shapes to labels\n",
    "labels = shapes.to_labels(image.shape) if nrofdim == 2 else np.zeros(image.shape,dtype=np.uint8)\n",
    "if nrofdim == 3:\n",
    "    labels[slicenumber,:,:] = shapes.to_labels(image.shape[1:3])  \n",
    "\n",
    "# Add the labels to the classification image\n",
    "class_image[labels > 0] = classnumber"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede27e2-584e-46c5-a739-b39af8982577",
   "metadata": {},
   "source": [
    "# 5. Postprocessing\n",
    "## 5.1. Creation of 2-D image from 3-D data\n",
    "When analysing 3D data, it may be useful to project the 3D segmentation onto a 2D map, e.g. for use in a GIS. To create a 2D map from a 3D label volume, please run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "213b12c8-6fd3-4257-8dc2-6a257b183229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'class_image_slice [2]' at 0x1c0ca8a1f40>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the class_image_slice array\n",
    "class_image_slice = np.zeros(image.shape[1:3], dtype=np.uint8)\n",
    "\n",
    "for m in range(image.shape[1]):\n",
    "    for n in range(image.shape[2]):\n",
    "        class_image_column = class_image[:, m, n]\n",
    "        non_zero_elements = class_image_column[class_image_column != 0]\n",
    "        \n",
    "        # Assign the values\n",
    "        if non_zero_elements.size == 0:\n",
    "            class_image_slice[m, n] = 0\n",
    "        elif (non_zero_elements.size) % 2 == 0:\n",
    "            class_image_slice[m, n] = np.median(non_zero_elements[:-1])\n",
    "        else:\n",
    "            class_image_slice[m, n] = np.median(non_zero_elements)\n",
    "\n",
    "# Add labels to the viewer\n",
    "viewer.add_labels(class_image_slice, opacity=0.65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ab29c-3a7e-47c9-a6cc-2db949d156f6",
   "metadata": {},
   "source": [
    "## 5.2. Creation of separate labels layers for each class\n",
    "A text file with the class names needs to be provided. In this file, the text must be of the form: {\"1\": \"Ditch\", \"2\": \"Road\", \"3\": \"House\"}. In the default code below, this text file is named 'Classes.txt'. If it is named differently, please specify its name below. It should be in the same folder as the Jupyter Notebook, or otherwise its path should be specified in the code below.<br> Within the folder where the input data is located, a 'Classes' folder will be created (first code cell below). In that folder, for each class a file with the objects belonging to that class will be stored (second code cell below). When working with 3D data, please indicate in the first line of the second code cell below if the files representing individual classes should be 2D (i.e. extracted from the 2D map created in step 5.1) or 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afc403a8-4674-4e19-90fa-8ba1ba5672a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file with the class names\n",
    "with open(\"Classes.txt\",\"r\") as file:\n",
    "    classes = json.loads(file.read())\n",
    "\n",
    "# Make a folder named 'Classes'\n",
    "pathname = os.path.dirname(viewer.layers[0].source.path)\n",
    "pathnameClasses = pathname + '/Classes/'\n",
    "pathnameClassesExist = os.path.exists(pathnameClasses)\n",
    "if not pathnameClassesExist:\n",
    "    os.makedirs(pathnameClasses)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b877a44d-3536-4a85-8308-1376a7d3ab3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filesIn3D = 'N' # For 3D data: make 2D class layers ('N') or 3D class layers ('Y') \n",
    "class_image = viewer.layers['class_image'].data\n",
    "for m in range(len(classes)):\n",
    "    classnr = int(list(classes)[m])\n",
    "    if nrofdim == 2:\n",
    "        objectclass = np.zeros(image.shape, dtype=np.uint8)\n",
    "        objectclass[class_image == classnr] = classnr\n",
    "    else:\n",
    "        if filesIn3D == 'N':\n",
    "            objectclass = np.zeros(image.shape[1:3], dtype=np.uint8)\n",
    "            objectclass[class_image_slice == classnr] = classnr            \n",
    "        else:\n",
    "            objectclass = np.zeros(image.shape, dtype=np.uint8)\n",
    "            objectclass[class_image == classnr] = classnr\n",
    "    mstr = str(classnr)\n",
    "    classname = classes[mstr]\n",
    "    viewer.add_labels(objectclass, opacity = 0.8, name = classname)\n",
    "    fullname = pathnameClasses + classname + '.tif'\n",
    "    viewer.layers[classname].save(fullname)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4043b328-78fb-4885-b556-d43e9652aef6",
   "metadata": {},
   "source": [
    "## 5.3. Morhological operations\n",
    "With the code below, morphological operations can be performed. First, indicate the number of the class to which the operation should be applied and the radius of the structuring element. Other structuring elements can be used, see https://scikit-image.org/docs/stable/api/skimage.morphology.html. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa594774-52c4-49eb-8f88-bd2426f9ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "classnumber = 3 # Define the class to which the operation should be applied\n",
    "classname = classes[str(classnumber)]\n",
    "original = viewer.layers[classname].data\n",
    "\n",
    "# Create structuring element\n",
    "radius = 7\n",
    "if nrofdim == 2:\n",
    "    footprint = morphology.disk(radius)\n",
    "else:\n",
    "    if filesIn3D == 'N':\n",
    "        footprint = morphology.disk(radius)\n",
    "    else:\n",
    "        footprint = morphology.ball(radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb1b238-e615-407d-b79a-edbf3163c0ec",
   "metadata": {},
   "source": [
    "Four operations can be performed: morphological dilation, erosion, closing and opening. Specify the operation in the first cell below. For other operations, see https://scikit-image.org/docs/stable/api/skimage.morphology.html. The result of the operation is saved in the 'Classes' folder created in section 5.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd6326b4-cdb5-42db-9280-2ac74ed67ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "operation = 'closing' # please specify the operation: 'dilation', 'erosion', 'opening' or 'closing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8a378bff-224e-4917-96f6-95120ed53823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable 'original' can be replaced by another variable, e.g. 'processed' to use the result of a previous operation\n",
    "if operation == 'dilation':\n",
    "    processed = morphology.dilation(original, footprint) \n",
    "elif operation == 'erosion':\n",
    "    processed = morphology.erosion(original, footprint)\n",
    "elif operation == 'opening':\n",
    "    processed = morphology.opening(original, footprint)\n",
    "elif operation == 'closing':\n",
    "    processed = morphology.closing(original, footprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b4f1476-ae49-478e-91d8-84cd27aaf8f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['J:/Archeologie/Falerii_Novi_2022/Publicaties/TBD/Zenodo/Classes/Linear_feature_morph_closing.tif']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add the result to the napari viewer and save it as a .tif file\n",
    "viewer.add_labels(processed, opacity = 0.8, name = classname + '_morph_' + operation)\n",
    "fullname = pathnameClasses + classname + '_morph_' + operation + '.tif'\n",
    "viewer.layers[classname + '_morph_' + operation].save(fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be618bc-ab54-4769-8211-fd0d1b2df6d7",
   "metadata": {},
   "source": [
    "# 6. Convert raster images to polygons, and save them as shapefiles\n",
    "If the input image is a GeoTIFF, the following information is automatically extracted by running the code in the first cell below:\n",
    "1. x-coordinate (Easting) and y-coordinate (Northing) of the lower left corner of the input data;\n",
    "2. sample distance in x- and y-direction (in m);\n",
    "3. coordinate reference system (EPSG code: https://epsg.org/home.html). <br>\n",
    "\n",
    "If the input image is not a GeoTIFF an error message will be given.\n",
    "The above information should then be entered manually in the second cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070204a4-9d3d-4323-9212-2cd59b8eda5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on: https://stackoverflow.com/questions/79059686/read-xyz-geotiff-file-with-tifffile\n",
    "with tifffile.TiffFile(path) as tif:\n",
    "    tif_tags = {}\n",
    "    for tag in tif.pages[0].tags.values():\n",
    "        name, value = tag.name, tag.value\n",
    "        tif_tags[name] = value\n",
    "coordx = tif_tags['ModelTiepointTag'][3]\n",
    "coordy = tif_tags['ModelTiepointTag'][4] - tif_tags['ImageLength'] * tif_tags['ModelPixelScaleTag'][1]\n",
    "sampledistx = tif_tags['ModelPixelScaleTag'][0]\n",
    "sampledisty = tif_tags['ModelPixelScaleTag'][1]\n",
    "CoordRefSyst = 'EPSG:' + str(tif_tags['GeoKeyDirectoryTag'][15])\n",
    "print('The following information has been extracted:\\nThe Easting and Northing of the lower left corner are (' + str(coordx) + ',' + str(coordy) + ').\\nThe sample distance in x and y direction is ' + str(sampledistx) + ' and ' + str(sampledisty) + ' m, respectively.\\nThe EPSG is ' + str(CoordRefSyst) + '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a810b9-0d64-4ab2-a25e-670516734be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordx = 395750 \n",
    "coordy = 4586831 \n",
    "sampledistx = 0.05 \n",
    "sampledisty = 0.05 \n",
    "CoordRefSyst = \"EPSG:32633\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e20886a-34fb-489a-83fc-8b887bc476c4",
   "metadata": {},
   "source": [
    "To run the code below, a text file mentioning the classes should be provided. In this file, the text should be of the form: {\"1\": \"Ditch\", \"2\": \"Road\", \"3\": \"House\"}. In the default code below, this text file is named 'Classes_test.txt'. If it is named differently, please specify its name below. It should be in the same folder as the Jupyter Notebook, or otherwise its path should be specified in the code below. <br> A subfolder called 'Shapefiles' will be created in the folder where input data is located. For each class, a shapefile with the name of the class is created. The path and file names can be changed by modifying the 'pathname' and 'fullname' variables in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fb2b13a-fc60-4c32-926c-3608244bc16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Classes.txt\",\"r\") as file:\n",
    "    classes = json.loads(file.read())\n",
    "pathname = os.path.dirname(viewer.layers[0].source.path)\n",
    "pathnameshp = pathname + '/Shapefiles/'\n",
    "pathnameshpExist = os.path.exists(pathnameshp)\n",
    "if not pathnameshpExist:\n",
    "    os.makedirs(pathnameshp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da7074c-e923-43e7-a632-33f26180cf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is based on: \n",
    "# https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html\n",
    "# https://docs.opencv.org/4.x/d9/d8b/tutorial_py_contours_hierarchy.html\n",
    "# https://stackoverflow.com/questions/57965493/how-to-convert-numpy-arrays-obtained-from-cv2-findcontours-to-shapely-polygons\n",
    "# https://gis.stackexchange.com/questions/353057/how-to-create-a-shapely-polygon-with-a-hole\n",
    "# https://shapely.readthedocs.io/en/stable/reference/shapely.MultiPolygon.html\n",
    "# https://shapely.readthedocs.io/en/2.0.3/manual.html\n",
    "\n",
    "for r in range(len(classes)):\n",
    "    classnr = list(classes)[r]   \n",
    "    classname = classes[classnr]\n",
    "    classbinarymap = viewer.layers[classname].data\n",
    "    classbinarymap[classbinarymap == int(classnr)] = 1   \n",
    "    classbinarymap2 = np.zeros(classbinarymap.shape, dtype=np.uint8)\n",
    "    \n",
    "    # Label connected components\n",
    "    classbinarymaplabeled = label(classbinarymap, connectivity=1)\n",
    "    \n",
    "    # Compute region properties\n",
    "    stats = regionprops(classbinarymaplabeled)\n",
    "    \n",
    "    # Filter bounding boxes\n",
    "    for q, s in enumerate(stats):\n",
    "        box = s.bbox\n",
    "        if box[2] - box[0] > 1 and box[3] - box[1] > 1:\n",
    "            classbinarymap2[classbinarymaplabeled == q + 1] = 1\n",
    "    \n",
    "    # Find contours\n",
    "    #contours, _ = cv.findContours(classbinarymap2, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "    contours, hierarchy = cv.findContours(classbinarymap2, cv.RETR_CCOMP, cv.CHAIN_APPROX_NONE)\n",
    "    #contoursSq = map(np.squeeze, contours)\n",
    "\n",
    "    # Convert contours to polygons. Take into account possible holes in polygons using the hierarchy of the contours\n",
    "    listofpolygons = []\n",
    "    for k in range(len(contours)):\n",
    "    \tif hierarchy[0][k][3] == -1:\n",
    "    \t\texterior = []\n",
    "    \t\tholes = []\n",
    "    \t\tfor m in range(len(contours[k])):\n",
    "    \t\t\ttuple_element = (contours[k][m][0][0],contours[k][m][0][1])\n",
    "    \t\t\texterior.append(tuple_element)\n",
    "    \t\tfor p in range(len(contours)):\n",
    "    \t\t\tif hierarchy[0][p][3] == k:\n",
    "    \t\t\t\tinterior = []\n",
    "    \t\t\t\tfor n in range(len(contours[p])):\n",
    "    \t\t\t\t\ttuple_element = (contours[p][n][0][0],contours[p][n][0][1])\n",
    "    \t\t\t\t\tinterior.append(tuple_element)\n",
    "    \t\t\t\t\tinterior[::-1]\n",
    "    \t\t\t\tholes.append(interior)\n",
    "    \t\tpgn = Polygon(exterior,holes)\n",
    "    \t\tlistofpolygons.append(pgn)\n",
    "    multipolygon = MultiPolygon(listofpolygons)\n",
    "       \n",
    "    # Apply geometric transformations\n",
    "    multipolygonflipud1 = affine_transform(multipolygon,[1, 0, 0, -1, 0, 0])\n",
    "    if nrofdim == 2:\n",
    "        multipolygonflipud2 = translate(multipolygonflipud1, yoff=image.shape[0])\n",
    "    elif nrofdim == 3:\n",
    "        multipolygonflipud2 = translate(multipolygonflipud1, yoff=image.shape[1])\n",
    "    multipolygonscaled = scale(multipolygonflipud2, xfact=sampledistx, yfact=sampledisty, origin=(0,0))    \n",
    "    multipolygontransl = translate(multipolygonscaled, xoff=coordx, yoff=coordy)\n",
    "    \n",
    "    # Create GeoDataFrame\n",
    "    gdf = gpd.GeoDataFrame(index=[0], geometry=[multipolygontransl], crs=CoordRefSyst)\n",
    "    \n",
    "    # Explode multipolygon into individual geometries\n",
    "    gdf_exploded = gdf.explode()\n",
    "    \n",
    "    # Save shapefile    \n",
    "    fullname = pathnameshp + classname + '.shp'\n",
    "    gdf_exploded.to_file(fullname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c43aa1-5c12-4234-a3df-254a2af61393",
   "metadata": {},
   "source": [
    "# 7. Tools\n",
    "## 7.1. Simple thresholding\n",
    "A simple threshold can be applied to the geophysical image instead of machine learning-based pixel classification and hysteresis thresholding (section 2). The threshold can be defined by the user or found using a particular method, e.g. Otsu's method (https://scikit-image.org/docs/stable/api/skimage.filters.html#skimage.filters.threshold_otsu)\n",
    "After running the code below, object classification can be performed (section 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d68eac24-119d-48ed-8bb4-6c6c66425c95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Labels layer 'Objects (Class 2) [1]' at 0x1f18e6e2580>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = 1.75\n",
    "# thresh = filters.threshold_otsu(image) \n",
    "binary = image > thresh\n",
    "viewer.add_labels(binary, opacity = 0.7)\n",
    "labeled_binary = label(binary, connectivity = 2, background = 0)\n",
    "viewer.add_labels(labeled_binary, name = \"Objects (Class 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7b092-3799-4743-93e6-18ac81670119",
   "metadata": {},
   "source": [
    "## 7.2. Correcting the boundaries of magnetometer anomalies\n",
    "For the correction, the horizontal gradient magnitude of the magnetometer data is calculated. For each anomaly, of the magnetometer data corresponding to the highest horizontal gradients, the mean is used as the threshold to delineate the corrected anomaly (please see the paper and the supplementaty data accompanying it). This operation needs the following input:\n",
    "1. the geophysical image,\n",
    "2. the labeled objects after hysteresis thresholding (section 2.8.3 or 3.1),\n",
    "3. the object classification: machine learning based (section 3) or manual (section 4).\n",
    "\n",
    "The result is a new classification map, with the boundaries of the objects corrected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "444cc040-94e7-42cc-959a-c60c95a70d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image = viewer.layers['class_image'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cf89bc-3700-4949-8c4d-51e5ef9b258e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_image_corrected = AnomalyEdge.find_edge_of_anomaly(image, labeled_hyst, class_image)\n",
    "viewer.add_labels(class_image_corrected, opacity = 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950edf68-e403-4c98-bc38-6cf8683fb173",
   "metadata": {},
   "source": [
    "## 7.3. Comparison of segmentations by means of quantitative metrics\n",
    "The result of the segmentation can be compared agaist a reference or 'ground truth' by means of different metrics. See https://scikit-learn.org/stable/api/sklearn.metrics.html and https://rasbt.github.io/mlxtend"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b98be63-b005-4401-bd21-a1e5a8ecb6d5",
   "metadata": {},
   "source": [
    "### 7.3.1. Confusion matrix\n",
    "In the code below, 'y_true' is the reference classification map, 'y_pred' is the prediction made by applying the steps above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b2320e-2ea0-44e7-98c8-948358d528c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be08930a-b0c6-4899-bcca-f8df9077c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please replace the class names in 'class_dict' by the classes in the segmentation maps to be compared.\n",
    "class_dict = {0: 'Background',\n",
    "              1: 'Circular',\n",
    "              2: 'Linear'}\n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm,colorbar=True,class_names=class_dict.values())\n",
    "\n",
    "# The code below uses a log-normalised colourmap. \n",
    "fig, ax = plot_confusion_matrix(conf_mat=cm,norm_colormap=matplotlib.colors.LogNorm(),colorbar=True,class_names=class_dict.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078008d-acf2-449d-a921-42f31ae2f688",
   "metadata": {},
   "source": [
    "### 7.3.2. Intersection over Union \n",
    "The Jaccard score, Jaccard index or Intersection over Union (IoU) metric is found by taking the intersection between a class in the reference classification and the same class in the segmentation map to be evaluated. It is divided by the union of the class in the reference and in the segmentation map to be evaluated. The IoU for each class can be returned (fist line of code below), or the mean IoU can be calculated, in different ways (see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f994b58-7b14-485e-a9b4-79637a82025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "js = jaccard_score(y_true, y_pred, average=None)\n",
    "js = jaccard_score(y_true, y_pred, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c00f7bd-b5a4-4312-8fcc-4ace4e7d9abf",
   "metadata": {},
   "source": [
    "### 7.3.3. F1 score\n",
    "The F1 score is equal to (2 * TP)/(2 * TP + FP + FN), where TP is the number of true positives, FN is the number of false negatives, and FP is the number of false positives. The F1 score can be given for each class separately (code below), or it can be averaged in different ways (see https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bdbb92-eeef-4b3e-a503-b24e1fb6b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1score = f1_score(y_true, y_pred, average=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d0f83d-aa58-4b7b-b873-b7c9bbf69a72",
   "metadata": {},
   "source": [
    "## 7.4. Send data from this notebook to napari viewer, and vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b092bcd4-7d2b-4412-b0a8-6a9241e4f73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Image layer 'DisplayedImage' at 0x1c0c5ab7340>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data from a layer in the napari viewer into this notebook\n",
    "importeddata = viewer.layers['LayerToBeImported'].data # change the names as needed\n",
    "\n",
    "# Display data in napari viewer\n",
    "# Display Labels or segmentation\n",
    "viewer.add_labels(dataToBeDisplayed, opacity = 0.7, name = 'DisplayedLayer') # change names and opacity as needed\n",
    "\n",
    "# Display an image\n",
    "viewer.add_image(imageToBeDisplayed, opacity = 0.7, name = 'DisplayedImage') # change names and opacity as needed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
